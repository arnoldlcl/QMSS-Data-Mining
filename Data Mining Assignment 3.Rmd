---
title: "QMSS G4058 Assignment 3"
author: "Arnold Lau"
date: "November 6, 2015"
output: html_document
---

### 1. Optimization

```{r}
training <- readRDS(gzcon(url("https://courseworks.columbia.edu/x/pJdP39")))
testing <- readRDS(gzcon(url("https://courseworks.columbia.edu/x/QnKLgY")))
```

Original "best" model from the midterm:

```{r}
ols <- lm(totalprice ~ area + zone + category + age + floor + rooms + out + conservation + toilets
                       + garage + elevator + streetcategory + heating + storage, data = training)
ols_step <- step(ols, trace = 0) # do not print each step
best_model <- ols_step

summary(best_model)
```

Function for sum of squared residuals:  

```{r}

train_X <- model.matrix(best_model)

ssr <- function(coef_proposals, observed_X_values, response_values) {
  # Inputs a matrix of observed values of the predictor variables, a numeric vector of proposed values for
  # coefficients, and a numeric vector of the actual values of the response variable
  # Outputs the sum of squared residuals
  
  if (length(observed_X_values[1, ]) != length(coef_proposals)) {
    stop("Vector of proposed values must be of the same length as the number of coefficents in the model, 
         including the intercept")
  }
  predicted_values <- (observed_X_values %*% coef_proposals)
  return(sum((response_values - predicted_values) ** 2))
}

# Test the function using the coefficients from the original model

all.equal(sum(best_model$residuals**2), ssr(as.numeric(coef(best_model)), train_X, training$totalprice))
```

Use the optim() function to find parameters that minimize the sum of squared residuals:

```{r}
result <- optim(rnorm(40, 0, 1), ssr, observed_X_values = train_X, response_values = training$totalprice, 
                method = "BFGS")
result$value                 # returned by general-purpose optim
sum(best_model$residuals**2) # returned by lm()
```

### 2. Principal Components Regression

```{r}
stopifnot(require(pls))
pcr1 <- pcr(totalprice ~ ., data = training)
pcr1_pred <- predict(pcr1, newdata = testing)

avg_sq_error <- function(pred_values, actual_values) {
  return(mean((actual_values - pred_values) ** 2))
}

# Create a vector of average squared errors for each value of k, k = # of principal components
pcr1_errors <- apply(pcr1_pred, MARGIN = 3, avg_sq_error, actual_values = testing$totalprice)
best_pcr_error <- pcr1_errors[which.min(pcr1_errors)]

# Original average squared error from the model used in the midterm:
y_hat <- predict(ols_step, newdata = testing)
best_error <- avg_sq_error(y_hat, testing$totalprice)

best_pcr_error
best_error
```

Using principal components regression with __k = 44__ yields the best predictive model under the average squared error criterion. This average squared error (819654757) is lower than the average squared error for the model I estimated on the midterm (864167580).

### 3. Data Mining with a Binary Outcome

```{r}
load("dataset.RData")
```

Divide the dataset into mutually exclusive training and testing data:  
```{r}
set.seed(8641)                               # Make the training/testing split reproducible
train_test <- rep("train", 6631)
train_test[sample((1:6631), 1326)] <- "test" # 80/20 split between training and testing data
dataset <- cbind(dataset, train_test)
training <- dataset[dataset$train_test == "train", ]
testing <- dataset[dataset$train_test == "test", ]
training$train_test <- NULL
testing$train_test <- NULL
```

Make a scatterplot of two of the first 18 variables that are both numeric and plausibly relevant to the probability that the borrower will have difficulty repaying the loan on time:  

```{r}
library(ggplot2)

# Plot loan amount and 
qplot(loan_amnt, revol_bal, data = training, col = home_ownership, pch = as.factor(y))
```

It seems plausible to me that higher loan amounts and higher revolving balance (the latter contributing to poorer credit scores) would be relevant to the probability that the borrower will have difficulty repaying the loan on time.  

Initial logit model:

```{r}
logit <- glm(y ~ loan_amnt + revol_bal + home_ownership, data = training, family = binomial)
y_hat_logit <- predict(logit, newdata = testing, type = "response")
cl_logit <- as.integer(y_hat_logit > 0.5)
table(testing$y, cl_logit) # Classifies 1119/1326 observations correctly
```

This model is effectively the same as just assuming nobody defaults.  

```{r}
# Isolating specific dummy variables
training$small_business <- NA
training$small_business[training$purpose == "small_business"] <- 1
training$small_business[training$purpose != "small_business"] <- 0
testing$small_business <- NA
testing$small_business[testing$purpose == "small_business"] <- 1
testing$small_business[testing$purpose != "small_business"] <- 0

training$unemployed <- NA
training$unemployed[is.na(training$emp_length)] <- 1
training$unemployed[!is.na(training$emp_length)] <- 0
testing$unemployed <- NA
testing$unemployed[is.na(testing$emp_length)] <- 1
testing$unemployed[!is.na(testing$emp_length)] <- 0

logit3 <- glm(as.factor(y) ~ loan_amnt + revol_bal + unemployed + inq_last_6mths + small_business + delinq_2yrs, data = training, family = binomial)
y_hat_logit3 <- predict(logit3, newdata = testing, type = "response")
cl_logit3 <- as.integer(y_hat_logit3 > 0.5)
table(testing$y, cl_logit3) # Classifies 1119/1326 observations correctly
```

This model also is pretty much just the same as assuming nobody defaults.

```{r}
library(glmpath)
X <- model.matrix(logit3)
X_test <- model.matrix(y ~ loan_amnt + revol_bal + unemployed + inq_last_6mths + small_business + delinq_2yrs, data = testing)
y <- training$y
path1 <- glmpath(X, y, nopenalty.subset = 1, family = binomial(link = "logit"))
summary(path1) # Step 11 has the lowest AIC
y_hat_path1 <- predict(path1, newx = X_test, type = "response", s = 11)
cl_path1 <- as.integer(y_hat_path1 > 0.5)
table(testing$y, cl_path1)
```

So is this model. I also tested dozens of models with different variables, with LPM, logit, LDA and QDA, most of which either performed at baseline or worse than baseline. Random forests didn't get me anywhere either. I'll just take logit3 to be the best model. (Maybe we should have the full Lending Club data instead of just a subset of features...)

```{r}
best_model <- logit3
try(next_year$small_business <- NA)
try(next_year$small_business[next_year$purpose == "small business"] <- 1)
try(next_year$small_business[next_year$purpose != "small business"] <- 0)
try(next_year$unemployed <- NA)
try(next_year$unemployed[is.na(next_year$emp_length)] <- 1)
try(next_year$unemployed[!is.na(next_year$emp_length)] <- 0)
try(y_hat_logit3_nxtyr <- predict(logit3, newdata = next_year, type = "response"))
try(cl_logit3_nxtyr <- as.integer(y_hat_logit3_nxtyr > 0.5))
try(table(next_year$y, cl_logit3_nxtyr))
```