---
title: "QMSS G4058 Assignment 3"
author: "Arnold Lau"
date: "November 6, 2015"
output: html_document
---

### 1. Optimization

```{r}
training <- readRDS(gzcon(url("https://courseworks.columbia.edu/x/pJdP39")))
testing <- readRDS(gzcon(url("https://courseworks.columbia.edu/x/QnKLgY")))
```

Original "best" model from the midterm:

```{r}
ols <- lm(totalprice ~ area + zone + category + age + floor + rooms + out + conservation + toilets
                       + garage + elevator + streetcategory + heating + storage, data = training)
ols_step <- step(ols, trace = 0) # do not print each step
best_model <- ols_step

summary(best_model)
```

Function for sum of squared residuals:  

```{r}

train_X <- model.matrix(best_model)

ssr <- function(coef_proposals, observed_X_values, response_values) {
  # Inputs a matrix of observed values of the predictor variables, a numeric vector of proposed values for
  # coefficients, and a numeric vector of the actual values of the response variable
  # Outputs the sum of squared residuals
  
  if (length(observed_X_values[1, ]) != length(coef_proposals)) {
    stop("Vector of proposed values must be of the same length as the number of coefficents in the model, 
         including the intercept")
  }
  predicted_values <- (observed_X_values %*% coef_proposals)
  return(sum((response_values - predicted_values) ** 2))
}

# Test the function using the coefficients from the original model

all.equal(sum(best_model$residuals**2), ssr(as.numeric(coef(best_model)), train_X, training$totalprice))
```

Use the optim() function to find parameters that minimize the sum of squared residuals:

```{r}
result <- optim(rnorm(40, 0, 1), ssr, observed_X_values = train_X, response_values = training$totalprice, 
                method = "BFGS")
result$value                 # returned by general-purpose optim
sum(best_model$residuals**2) # returned by lm()
```

### 2. Principal Components Regression

```{r}
stopifnot(require(pls))
pcr1 <- pcr(totalprice ~ ., data = training)
pcr1_pred <- predict(pcr1, newdata = testing)

avg_sq_error <- function(pred_values, actual_values) {
  return(mean((actual_values - pred_values) ** 2))
}

# Create a vector of average squared errors for each value of k, k = # of principal components
pcr1_errors <- apply(pcr1_pred, MARGIN = 3, avg_sq_error, actual_values = testing$totalprice)
best_pcr_error <- pcr1_errors[which.min(pcr1_errors)]

# Original average squared error from the model used in the midterm:
y_hat <- predict(ols_step, newdata = testing)
best_error <- avg_sq_error(y_hat, testing$totalprice)

best_pcr_error
best_error
```

Using principal components regression with __k = 44__ yields the best predictive model under the average squared error criterion. This average squared error (819654757) is lower than the average squared error for the model I estimated on the midterm (864167580).

### 3. Data Mining with a Binary Outcome

```{r}
load("dataset.RData")
```

Divide the dataset into mutually exclusive training and testing data:  
```{r}
set.seed(8641)                               # Make the training/testing split reproducible
train_test <- rep("train", 6631)
train_test[sample((1:6631), 1326)] <- "test" # 80/20 split between training and testing data
dataset <- cbind(dataset, train_test)
training <- dataset[dataset$train_test == "train", ]
testing <- dataset[dataset$train_test == "test", ]
training$train_test <- NULL
testing$train_test <- NULL
```

Make a scatterplot of two of the first 18 variables that are both numeric and plausibly relevant to the probability that the borrower will have difficulty repaying the loan on time:  

```{r}
library(ggplot2)

# Plot loan amount and 
qplot(loan_amnt, revol_bal, data = training, col = home_ownership, pch = as.factor(y))
```

Initial logit model:

```{r}
logit <- glm(y ~ loan_amnt + revol_bal + home_ownership, data = training, family = binomial)
y_hat_logit <- predict(logit, newdata = testing, type = "response")
classified_logit <- as.integer(y_hat_logit > 0.5)
table(testing$y, classified_logit) # Classifies 1119/1326 observations correctly
```

This model is completely useless - it's effectively the same as just assuming nobody defaults.  

```{r}
fit <- penalized(y, ~ loan_amnt + term + int_rate + installment + annual_inc +
                      verification_status + purpose + delinq_2yrs + earliest_cr_line + inq_last_6mths +
                      open_acc + pub_rec + revol_bal + total_acc, 
                data = training, lambda1 = 1, model = "logistic", standardize = TRUE)
pred <- predict(fit,  ~ loan_amnt + term + int_rate + installment + annual_inc +
                      verification_status + purpose + delinq_2yrs + earliest_cr_line + inq_last_6mths +
                      open_acc + pub_rec + revol_bal + total_acc, data = testing)
cl_fit <- as.integer(pred > 0.5)
table(testing$y, cl_fit)
